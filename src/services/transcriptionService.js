/**
 * Service de transcription audio avec Whisper API (OpenAI)
 * G√®re la transcription en temps r√©el avec streaming
 */

class TranscriptionService {
  constructor() {
    this.apiKey = import.meta.env?.VITE_OPENAI_API_KEY;
    this.apiUrl = 'https://api.openai.com/v1/audio/transcriptions';
    this.isRecording = false;
    this.mediaRecorder = null;
    this.audioChunks = [];
    this.onTranscriptCallback = null;
    this.language = 'fr';
    this.hasWhisperFailed = false; // Flag pour √©viter de r√©essayer Whisper
  }

  /**
   * Initialise la capture audio et d√©marre la transcription
   */
  async startTranscription(language = 'fr', onTranscript) {
    this.language = language;
    this.onTranscriptCallback = onTranscript;
    this.isRecording = true;

    try {
      // Demander permission microphone
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });

      // Utiliser Web Speech API par d√©faut (gratuit et fiable)
      console.log('üé§ Utilisation de Web Speech API (mode gratuit)');
      return this.startWebSpeechAPI(stream, language, onTranscript);
      
      // Code Whisper d√©sactiv√© pour privil√©gier Web Speech API
      // Pour r√©activer Whisper, d√©commentez le code ci-dessous
      /*
      if (this.hasWhisperFailed || !this.apiKey || this.apiKey === '' || this.apiKey === 'your_openai_api_key_here') {
        const reason = this.hasWhisperFailed ? 'quota d√©pass√©' : 'non configur√©e';
        console.warn(`‚ö†Ô∏è API OpenAI ${reason}, utilisation de Web Speech API en fallback`);
        console.log('Cl√© API actuelle:', this.apiKey ? '(pr√©sente)' : '(non d√©finie)');
        return this.startWebSpeechAPI(stream, language, onTranscript);
      }
      
      console.log('‚úÖ API OpenAI configur√©e, utilisation de Whisper');
      console.log('Cl√© API (d√©but):', this.apiKey ? this.apiKey.substring(0, 20) + '...' : 'non trouv√©e');
      */

      // MediaRecorder non utilis√© en mode Web Speech API
      // (Garder pour compatibilit√© future si Whisper est r√©activ√©)
      
      /*
      this.mediaRecorder = new MediaRecorder(stream);
      this.audioChunks = [];

      this.mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          this.audioChunks.push(event.data);
        }
      };

      // Code Whisper comment√© (non utilis√©)
      */

      return stream;

    } catch (error) {
      console.error('‚ùå Erreur lors du d√©marrage de la transcription:', error);
      throw error;
    }
  }

  /**
   * Envoie l'audio √† l'API Whisper
   */
  async sendToWhisper() {
    try {
      const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm' });
      
      // V√©rifier que l'audio a une taille suffisante (au moins 1KB)
      if (audioBlob.size < 1000) {
        console.log('‚è≠Ô∏è Audio trop court, ignor√©');
        return;
      }
      
      console.log('üì§ Envoi √† Whisper:', audioBlob.size, 'bytes');
      
      // Cr√©er FormData pour l'API
      const formData = new FormData();
      formData.append('file', audioBlob, 'audio.webm');
      formData.append('model', 'whisper-1');
      formData.append('language', this.language === 'fr' ? 'fr' : 'en');
      formData.append('response_format', 'json');

      // Appeler l'API Whisper
      const response = await fetch(this.apiUrl, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`
        },
        body: formData
      });

      if (!response.ok) {
        const errorText = await response.text();
        console.error('‚ùå Erreur API Whisper:', response.status, errorText);
        
        if (response.status === 429) {
          console.warn('‚ö†Ô∏è Rate limit atteint, attente avant prochain envoi...');
          // Attendre 2 secondes avant de permettre le prochain envoi
          await new Promise(resolve => setTimeout(resolve, 2000));
        }
        throw new Error(`Erreur API Whisper: ${response.status}`);
      }

      const result = await response.json();
      console.log('‚úÖ Transcription re√ßue:', result.text);
      
      // Callback avec le texte transcrit
      if (result.text && this.onTranscriptCallback) {
        this.onTranscriptCallback({
          text: result.text,
          timestamp: Date.now(),
          isFinal: true
        });
      }

    } catch (error) {
      console.error('‚ùå Erreur Whisper API:', error);
      
      // Si erreur de quota, marquer pour basculer
      if (error.message && (error.message.includes('429') || error.message.includes('quota'))) {
        console.warn('‚ö†Ô∏è Quota Whisper d√©pass√©, marquage pour basculement...');
        this.hasWhisperFailed = true;
      }
    }
  }

  /**
   * Fallback vers Web Speech API si Whisper n'est pas disponible
   */
  async startWebSpeechAPI(existingStream, language, onTranscript) {
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    
    if (!SpeechRecognition) {
      throw new Error('Web Speech API non support√©e dans ce navigateur');
    }

    // Si pas de stream existant, en cr√©er un nouveau
    if (!existingStream) {
      try {
        existingStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          }
        });
        // Stocker le stream pour pouvoir l'arr√™ter plus tard
        this.stream = existingStream;
      } catch (error) {
        console.error('Erreur acc√®s microphone pour Web Speech:', error);
        throw error;
      }
    }

    const recognition = new SpeechRecognition();
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.lang = language === 'fr' ? 'fr-FR' : 'en-US';

    recognition.onresult = (event) => {
      const last = event.results.length - 1;
      const text = event.results[last][0].transcript;
      const isFinal = event.results[last].isFinal;

      onTranscript({
        text: text,
        timestamp: Date.now(),
        isFinal: isFinal
      });
    };

    recognition.onerror = (event) => {
      console.error('Erreur Web Speech API:', event.error);
      if (event.error === 'no-speech') {
        recognition.start(); // Red√©marrer
      }
    };

    recognition.onend = () => {
      if (this.isRecording) {
        recognition.start(); // Red√©marrer pour transcription continue
      }
    };

    recognition.start();
    
    // Stocker la r√©f√©rence
    this.recognition = recognition;

    return existingStream;
  }

  /**
   * Met en pause la transcription
   */
  pause() {
    this.isRecording = false;
    if (this.mediaRecorder && this.mediaRecorder.state === 'recording') {
      this.mediaRecorder.stop();
    }
    if (this.recognition) {
      this.recognition.stop();
    }
  }

  /**
   * Reprend la transcription
   */
  resume() {
    this.isRecording = true;
    if (this.mediaRecorder && this.mediaRecorder.state !== 'recording') {
      this.mediaRecorder.start();
      setTimeout(() => {
        if (this.isRecording && this.mediaRecorder.state === 'recording') {
          this.mediaRecorder.stop();
        }
      }, 10000); // 10 secondes
    }
    if (this.recognition) {
      this.recognition.start();
    }
  }

  /**
   * Arr√™te la transcription
   */
  stop() {
    this.isRecording = false;
    if (this.mediaRecorder) {
      this.mediaRecorder.stop();
      this.mediaRecorder.stream.getTracks().forEach(track => track.stop());
    }
    if (this.recognition) {
      this.recognition.stop();
    }
    if (this.stream) {
      this.stream.getTracks().forEach(track => track.stop());
    }
  }
}

// Singleton
const transcriptionService = new TranscriptionService();
export default transcriptionService;
