import { useState, useEffect, useRef } from 'react';
import transcriptionService from '../services/transcriptionService';

export default function ActiveSession({ config, onEnd }) {
  const [transcript, setTranscript] = useState([]);
  const [duration, setDuration] = useState(0);
  const [isRecording, setIsRecording] = useState(false);
  const [isPaused, setIsPaused] = useState(false);
  const [micStatus, setMicStatus] = useState('Initialisation...');
  const [demoMode, setDemoMode] = useState(false);
  const mediaRecorderRef = useRef(null);
  const recognitionRef = useRef(null);
  const transcriptEndRef = useRef(null);
  const demoIntervalRef = useRef(null);

  useEffect(() => {
    startRecording();
    const timer = setInterval(() => {
      if (!isPaused) {
        setDuration(d => d + 1);
      }
    }, 1000);

    return () => {
      clearInterval(timer);
      stopRecording();
      if (demoIntervalRef.current) {
        clearInterval(demoIntervalRef.current);
      }
    };
  }, []);

  const startDemoMode = () => {
    setDemoMode(true);
    setMicStatus('ğŸ­ MODE DÃ‰MO - Transcription simulÃ©e pour tests');
    
    const demoTexts = [
      "Bonjour Ã  tous, bienvenue dans cette rÃ©union",
      "Aujourd'hui nous allons discuter du projet CORTEXIA",
      "Il faut terminer l'architecture technique d'ici vendredi",
      "Nous avons dÃ©cidÃ© de valider l'approche proposÃ©e",
      "L'Ã©quipe de dÃ©veloppement va commencer les tests",
      "Le planning est confirmÃ© pour la semaine prochaine",
      "Il est important de finaliser la documentation",
      "Nous devons organiser une dÃ©monstration client",
      "Les prochaines Ã©tapes sont clairement dÃ©finies",
      "Merci pour votre participation Ã  cette session"
    ];
    
    let index = 0;
    demoIntervalRef.current = setInterval(() => {
      if (!isPaused && index < demoTexts.length) {
        setTranscript(prev => [...prev, {
          id: Date.now(),
          timestamp: Date.now(),
          text: demoTexts[index],
          speaker: 'Participant',
          isFinal: true
        }]);
        setMicStatus(`ğŸ­ MODE DÃ‰MO - ${index + 1}/${demoTexts.length} segments gÃ©nÃ©rÃ©s`);
        index++;
      } else if (index >= demoTexts.length) {
        clearInterval(demoIntervalRef.current);
        setMicStatus('ğŸ­ MODE DÃ‰MO - Tous les segments gÃ©nÃ©rÃ©s');
      }
    }, 3000); // Un segment toutes les 3 secondes
  };

  useEffect(() => {
    // Auto-scroll vers le bas
    transcriptEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [transcript]);

  const startRecording = async () => {
    try {
      setMicStatus('ğŸ¤ Demande accÃ¨s microphone...');
      
      // VÃ©rifier si une clÃ© API Whisper est disponible
      const hasWhisperKey = import.meta.env.VITE_OPENAI_API_KEY;
      
      if (hasWhisperKey) {
        // Utiliser Whisper API
        setMicStatus('ğŸš€ Initialisation Whisper API...');
        
        await transcriptionService.startTranscription(
          config.language,
          (transcriptText) => {
            setTranscript(prev => [...prev, {
              id: Date.now(),
              timestamp: Date.now(),
              text: transcriptText,
              speaker: 'Participant',
              isFinal: true
            }]);
          }
        );
        
        setMicStatus('âœ… Whisper API actif - Transcription professionnelle');
        setIsRecording(true);
        return;
      }
      
      // Sinon utiliser Web Speech API
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });

      setMicStatus('âœ… Microphone connectÃ©');
      mediaRecorderRef.current = new MediaRecorder(stream);

      // Web Speech API
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      if (SpeechRecognition) {
        const recognition = new SpeechRecognition();
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.lang = config.language === 'fr' ? 'fr-FR' : 'en-US';

        recognition.onstart = () => {
          setMicStatus('ğŸ¤ Ã‰coute en cours... Parlez maintenant !');
          console.log('Recognition started');
        };

        recognition.onresult = (event) => {
          const last = event.results.length - 1;
          const text = event.results[last][0].transcript;
          const isFinal = event.results[last].isFinal;

          console.log('Transcription:', text, 'Final:', isFinal);

          if (isFinal) {
            setMicStatus('âœ… Transcription active');
            setTranscript(prev => [...prev, {
              id: Date.now(),
              timestamp: Date.now(),
              text: text.trim(),
              speaker: 'Participant',
              isFinal: true
            }]);
          } else {
            setMicStatus('ğŸ¤ En cours : ' + text.substring(0, 30) + '...');
          }
        };

        recognition.onerror = (event) => {
          console.error('Erreur reconnaissance:', event.error);
          if (event.error === 'no-speech') {
            setMicStatus('âš ï¸ Aucune voix dÃ©tectÃ©e - parlez plus fort');
            recognition.start();
          } else if (event.error === 'not-allowed') {
            setMicStatus('âŒ Permission microphone refusÃ©e');
            alert('âŒ AccÃ¨s au microphone refusÃ©. Autorisez le microphone dans les paramÃ¨tres.');
          } else if (event.error === 'network') {
            setMicStatus('ğŸ”„ Mode dÃ©mo activÃ© - Simulation de transcription');
            console.log('Web Speech API indisponible, passage en mode dÃ©mo');
            // Activer le mode simulation
            startDemoMode();
          } else {
            setMicStatus('âš ï¸ Erreur: ' + event.error);
          }
        };

        recognition.onend = () => {
          console.log('Recognition ended, restarting...');
          if (isRecording && !isPaused) {
            recognition.start();
          }
        };

        recognition.start();
        recognitionRef.current = recognition;
        setMicStatus('ğŸ¤ PrÃªt - Commencez Ã  parler');
      } else {
        setMicStatus('âŒ Web Speech API non supportÃ©');
        alert('âŒ Votre navigateur ne supporte pas la reconnaissance vocale');
      }

      setIsRecording(true);
    } catch (error) {
      console.error('Erreur capture audio:', error);
      setMicStatus('âŒ Erreur microphone');
      alert('âŒ Impossible d\'accÃ©der au microphone : ' + error.message);
    }
  };

  const stopRecording = () => {
    // ArrÃªter Whisper ou Web Speech
    if (transcriptionService.isRecording) {
      transcriptionService.stop();
    }
    
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop();
      mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());
    }
    if (recognitionRef.current) {
      recognitionRef.current.stop();
    }
    if (demoIntervalRef.current) {
      clearInterval(demoIntervalRef.current);
    }
    setIsRecording(false);
  };

  const handlePauseResume = () => {
    if (isPaused) {
      if (recognitionRef.current && !demoMode) {
        recognitionRef.current.start();
      }
      if (demoMode && demoIntervalRef.current) {
        // Le mode dÃ©mo continue automatiquement
      }
      setTranscript(prev => [...prev, {
        id: Date.now(),
        timestamp: Date.now(),
        text: 'â–¶ Session reprise',
        speaker: 'SystÃ¨me',
        isSystem: true
      }]);
    } else {
      if (recognitionRef.current && !demoMode) {
        recognitionRef.current.stop();
      }
      setTranscript(prev => [...prev, {
        id: Date.now(),
        timestamp: Date.now(),
        text: 'â¸ Session en pause',
        speaker: 'SystÃ¨me',
        isSystem: true
      }]);
    }
    setIsPaused(!isPaused);
  };

  const handleStop = () => {
    if (confirm('ğŸ›‘ Voulez-vous vraiment terminer cette session ?')) {
      stopRecording();
      onEnd(transcript, duration);
    }
  };

  const handleMarkMoment = () => {
    const note = prompt('ğŸ“Œ Note pour ce moment important :');
    if (note) {
      setTranscript(prev => [...prev, {
        id: Date.now(),
        timestamp: Date.now(),
        text: `ğŸ“Œ ${note}`,
        speaker: 'SystÃ¨me',
        marked: true
      }]);
    }
  };

  const formatDuration = (seconds) => {
    const hrs = Math.floor(seconds / 3600);
    const mins = Math.floor((seconds % 3600) / 60);
    const secs = seconds % 60;
    if (hrs > 0) {
      return `${hrs}:${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
    }
    return `${mins}:${secs.toString().padStart(2, '0')}`;
  };

  return (
    <div className="screen active-session">
      <div className="session-header">
        <div className="session-info">
          <h2>{config.title}</h2>
          <div className="recording-indicator">
            {!isPaused && <span className="red-dot"></span>}
            {isPaused ? 'â¸ï¸ En pause' : 'ğŸ”´ Enregistrement en cours'} â€¢ {formatDuration(duration)}
          </div>
          <div className="mic-status" style={{ fontSize: '14px', marginTop: '8px', opacity: 0.8 }}>
            {micStatus}
          </div>
        </div>
      </div>

      <div className="transcript-container">
        <div className="transcript-stats">
          {transcript.filter(t => t.isFinal).length} segments â€¢ 
          ğŸ“Œ {transcript.filter(t => t.marked).length} moments marquÃ©s
        </div>
        
        {transcript.length === 0 && (
          <div className="transcript-empty">
            <p>ğŸ¤ En attente de parole...</p>
            <small>Commencez Ã  parler pour voir la transcription apparaÃ®tre</small>
            <br />
            <small style={{ marginTop: '10px', display: 'block', color: '#666' }}>
              Statut: {micStatus}
            </small>
          </div>
        )}

        {transcript.map((item) => (
          <div 
            key={item.id} 
            className={`transcript-line ${item.marked ? 'marked' : ''} ${item.isSystem ? 'system' : ''}`}
          >
            <span className="timestamp">
              {new Date(item.timestamp).toLocaleTimeString('fr-FR', { 
                hour: '2-digit', 
                minute: '2-digit',
                second: '2-digit'
              })}
            </span>
            <span className="speaker">{item.speaker}:</span>
            <span className="text">{item.text}</span>
          </div>
        ))}
        <div ref={transcriptEndRef} />
      </div>

      <div className="session-controls">
        <button 
          onClick={handlePauseResume} 
          className={`btn-secondary ${isPaused ? 'btn-resume' : ''}`}
        >
          {isPaused ? 'â–¶ï¸ Reprendre' : 'â¸ï¸ Pause'}
        </button>
        
        <button onClick={handleMarkMoment} className="btn-secondary">
          ğŸ“Œ Marquer ce moment
        </button>
        
        <button onClick={handleStop} className="btn-danger">
          â¹ï¸ Terminer la session
        </button>
      </div>
    </div>
  );
}
